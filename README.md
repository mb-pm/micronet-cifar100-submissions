# Micronet CIFAR-100 Submissions

This README contains the information about our submissions, as well as instructions for evaluating and training
them.

## Submissions

* Candidate number 1: QuantBinSimpNet -> Path to model: `source/outputs/quant_bin_simpnet/best_model`
* Candidate number 2: QuantBinSimpNet+ -> Path to model: `source/outputs/quant_bin_simpnet_plus/best_model`

## Requirements

The only requirement is to have docker installed on the machine where the training/evaluation will be run.

## Training the model 

Training the model has one assumption - there exists a file with an augmentation schedule used for training. There
are two options - use our augmentation schedule or run script `train_pba.sh` which will generate file
`pba_augmentations.txt`. The same augmentation schedule is used for training both submission models.

To train the model the same way the candidate 1 was trained (QuantBinSimpNet), run script `train_candidate_1.sh`.
To train the model the same way the candidate 2 was trained (QuantBinSimpNet+), run script `train_candidate_2.sh`.
Both of the train scripts expect one argument - whether augmentation schedule trained by us should be used or the augmentation schedule generated by running the `train_pba.sh` script.
i.e. `train_candidate_1.sh 1` will use augmentation schedule trained by us, and `train_candidate_1.sh 0` will use 
the augmentation schedule which was generated by the previously mentioned script. The same applies to training of the other candidate
model.

The parameters for pba search are set to following: 500 epochs, 16 instances of Ray Models. By default, the number of instances
of cpu are 40 and number of gpus are 1. If there is not an adequate number of cpus/gpus please modify the `source/pba/search.py`
script accordingly.


## Evaluating the model

To evaluate the candidate 1 model (QuantBinSimpNet), run script `evaluate_candidate_1.sh`.
To evaluate the candidate 2 model (QuantBinSimpNet+), run script `evaluate_candidate_2.sh`.

Both of the evaluation scripts expect one argument that defines whether our best already trained model is evaluated or the newly trained model.
i.e. `evaluate_candidate_1.sh 1` will use best model trained by us, and `evaluate_candidate_1.sh 0` will use 
newly trained model from `train_candidate_1.sh` script. Same applies to evaluation of the other candidate model.


## Approach

The starting point of our approach was SimpNet ([paper](https://arxiv.org/pdf/1802.06205v1.pdf)). It's a simple architecture constructed from regular 3x3 convolution layers of which 
all have stride equal to 1. Along with convolutions there are two *SAF* Pooling layers, which are simply Max Pools 
followed by dropout immediately. In total, this net has 13 convolutional layers which are followed by 
global max pooling and a dense layer with softmax activation. The only change between our approach and Simpnet's paper
is that we reduced the number of filters in fifth convolutional layer from 192 to 128 filters.

The authors of the paper described two versions of the model, one has 5.5M parameters, while the other has 8.9M parameters.
They reported results on CIFAR-100 at 78.08% and 79.19% accuracy respectively. In order to close the gap and achieve above 80% accuracy we applied a better data-augmentation and training policy.

We used Population Based Augmentation ([paper](https://arxiv.org/pdf/1905.05393.pdf)) and
found the optimal policies for the augmentations on CIFAR-100 dataset. For training the augmentation schedule, we used
reduced CIFAR-100 dataset - only the train set of CIFAR-100 was used. For training of the sample models, we used
5000 images from the train set (to speed up the exploration process), and for the evaluation 
we used the remaining 45000 images from train set. 
 
We also had some minor but effective changes to the learning process, 
such as introducing Smooth Labels ([paper](https://arxiv.org/pdf/1906.02629)) and 
using Stochastic Weight Averaging ([paper](https://arxiv.org/pdf/1803.05407.pdf)) which boosted our results. Also, 
instead of using ReLU, we used ELU ([paper](https://arxiv.org/pdf/1511.07289.pdf)).
 
Our main part of the approach was quantization. We binarized all convolutional layers, or rather the weights of
convolutional layers according to Binary Connect [paper](https://arxiv.org/pdf/1511.00363.pdf). We also quantized
the ([reference](https://www.tensorflow.org/lite/performance/post_training_quantization)) inputs to each 
convolutional layer which significantly reduced the complexity of the operations and parameters. Our two submissions are based
on different granularity of quantization applied to inputs to the convolutional layers. First submission
quantizes inputs into 16 bits - this quantization was performed by simply casting inputs in each convolutional layer 
into float16. Second submission quantizes inputs into 9 bits - training of the model is quantization aware (it is 
trained with TensorFlow's fake quantization nodes which include optimization of min/max range of activations).

The hyperparameters for training of the model are as described in train scripts with the batch size being set to 128, 
optimizer being SGD with learning rate `0.1` which decreases by `0.65` every 30 epochs. Weights were regularized with l2 norm
with `1e-3` value. Hyperparameters were obtained by observing the training loss curves, their convergence speed and
stability.


## Counting

In all counting formulas, H represents the image/feature map height, W represents the image/feature map height and C
represents the number of image/feature map channels.

### Convolutional layers
Since all our convolutional layers (in both submissions) are binary with stride 1 and without biases, 
calculation is as follows:

* parameters are counted as 1/32 of parameter size since they can be represented in 1 bit
    * number of parameters per layer is K\*K\*C<sub>in</sub>\*C<sub>out</sub>/32, where K is kernel size,
    C<sub>in</sub> is number of input channels, and C<sub>out</sub> is number of output channels

* operations
    
    * multiplications
        * total number of multiplications is scaled with 1/32 since the multiplications 
        can be carried by modifying single bit
        * i.e. for convolution over HxWxC<sub>in</sub> feature map
        and kernel size KxK, number of multiplications is K\*K\*C<sub>in</sub>\*N/32, where N is the number of 
        output feature map elements (W\*H\*C<sub>out</sub>)
    
    * additions
        * since the multiplication operation is just changing the single bit (multiplication with -1 or 1),
        and that does not change precision of the input element (even in the
        kernels implemented in deep learning frameworks, so rounding the numbers to wanted precision is not needed
        after the multiplication operation), we assume that total number of additions is scaled with n/32, 
        where n is the input bitwidth to convolutional layer - in our first submission, bitwidth of inputs
        is 16, and in our second submission, it is 9 bits
        * i.e. for convolution over HxWxC<sub>in</sub> feature map
        and kernel size KxK, number of additions is (K\*K\*C<sub>in</sub>-1)\*N*n/32, where N is the number of 
        output feature map elements (W\*H\*C<sub>out</sub>), and n is the input bitwidth
        
### BatchNorm layers
BatchNorm layers are used only after each convolutional layer.
Since we use binary weighted convolutional layers, BatchNorm layers cannot be fused into them. However,
we fuse BatchNorm layer into single linear transformation (single multiply and add per input element).

* parameters of BatchNorm layers are not quantized in any way - they are in full, float32 format
    * number of parameters per layer is 2 * C where C is the number of channels
    of feature map on which BatchNorm is performed
* operations
    * multiplications
        * total number of multiplications performed equals to the number of total input elements in feature map - 
        H\*W\*C
    * additions
        * total number of additions performed equals to the number of total input elements in feature map - 
        H\*W\*C
        
### MaxPool layers
MaxPool layers are used after some convolutional layers. They are parameter-less, but they do cost 
multiplication operations (we count comparison as single multiplication operation).

* 0 parameters

* operations
    * multiplications
        * for every element of output feature map, we have to do K\*K-1 multiplications (comparisons)
        * i.e. for input feature map of size HxWxC, total number of multiplications (comparisons) needed is
        (K\*K-1)\*N, where N is the number of output feature map elements ((H/s)\*(W/s)\*C) - K is pooling
        kernel size, and s is pooling kernel stride (in our case, both K and s are 2 for all max pooling layers)
    * 0 additions
        
### GlobalMaxPool layers
GlobalMaxPool layer is, in our case, used after all convolutional layers, and before final dense layer 
that makes predictions. It is parameter-less, but it (similarly as MaxPool layers) costs multiplication 
operations (comparisons).

* 0 parameters

* operations
    * multiplications
        * for every channel of input feature map, we have to find maximum element (we have to do HxW-1 
        multiplications)
        * i.e. for input feature map of size HxWxC, total number of multiplications (comparisons) needed is
        (H\*W-1)\*C
    * 0 additions
    
    
### Dense layers
Both of our submissions contain single dense layer (per model). It is used only for predicting the class.
Input to the dense layer is GlobalMaxPooled output feature map from last convolutional layer which has
432 output channels.

* parameters
    * in both of our submissions, dense layer has 432\*100 parameters (they are full precision, float32)

* operations
    * 432\*100 + (100-1) multiplications
        * 100-1 multiplications count is for argmax operation on dense
        logits to get the index of predicted class
    * (432-1)\*100+100 additions (dense layer has biases, also of full precision, float32)
        
        
### Activation layer (ELU)
All of our convolutional layers use ELU (exponential linear unit). Exponential linear unit is defined as follows:
if x>=0, output is x, else output is e<sup>x</sup>-1

* 0 parameters

* operations
    * 2 multiplications (1 for comparison with zero, another one is e<sup>x</sup>)
    * 1 addition (-1 in e<sup>x</sup>-1)
    
    
### KvantizationLayer
In our second submission, before every convolutional layer, quantization is performed. We used TensorFlow's fake
quantization (tf.quantization.fake_quant_with_min_max_vars). Number of bits used for fake quantization is 7 since
we want to bucket our activation range \[min, max\] (per layer) into 128 different values. Quantization is performed in
 the following manner: 

1) We want to make sure that after quantization, we have a number representation which allows 1 bit multiplication
in our binary convolutions and fast, low-bitwidth multiplication in our binary convolutions
2) We want to make sure that quantized values have free conversion from/to full precision float32 values

Following are all theoretical assumptions - ideally, we would have something like signed magnitude representation
of sign + absolute value of quantized index and arithmetic unit that would optimally add those numbers. Additionally, 
our binary 1 bit multiplication would also be feasible because of the sign bit.

To make sure we satisfy these properties, we would make the quantization/dequantization for the
inference in the following manner (that way of quantizing/dequantizing produce same float32 dequantized values as 
does tf.quantization.fake_quant_with_min_max_vars layer - ordering is a bit different for our, theoretical inference
layer and TensorFlow quantization layer, but end results are the same):

* Quantization
    1) Fix \[min, max\] range same as TensorFlow does [documentation](https://www.tensorflow.org/api_docs/python/tf/quantization/fake_quant_with_min_max_vars)
    2) Clamp the input float32 value into \[min, max\] range
    3) Subtract the minimum value min from clamped float32 value
    4) Divide the value with step value - step value is equal to (max-min)/(n-1), where n is total number of possible
    quantized values (128 in our case)
    5) Round the value to get index of quantized value (in range from 0 to 127)
    6) Mask the clamped value 7 exponential bits with bitstring index of quantized value
    7) Use only first 8 bits of the number as quantized value

When float32 values are quantized this way, we are using sign bit, and 7 exponent bits of full float32 number.
All other numbers of full float32 value are not needed, so we can just clamp first 8 bits of quantized value 
(something like first 16 bits in bfloat16). Numbers quantized like that are still valid float numbers which can be used 
in convolutional layers and satisfy properties mentioned earlier (1 bit multiplication with binary weights because of
the bit sign, free conversion to/from float32 by clamping/adding 24 bits). Convolution is carried on number quantized to
8 bits - additions will also be in 8 bits because binary multiplication won't make any changes to quantized values (
except of the sign change).

* Dequantization
    1) Convert the number into the full float32 (by setting sign bit + 7 exponent bits from quantized 8 bit value) 
    2) Shift full float32 number right to extract unsigned quantization index value
    3) Multiply the extracted value with the step value - step value is equal to (max-min)/(n-1), where n is total number of possible
    quantized values (128 in our case)
    4) Multiply value from step 3) with the sign bit of value from step 1)
    5) Add the minimum value min to the multiplied value
    
* parameters
    * every KvantizationLayer has 3 parameters - min, max and step which are used in quantization and dequantization as
    * when calculating step value, n is fixed in all our KvantizationLayers to 7
* operations (all the operations are performed on full precision, float32 values)
    
    * quantization
        
        * multiplications (per value of feature map)
            * 2 multiplications for clamping the value into \[min, max\] range (2 comparisons)
            * 1 multiplication to divide the value with step value
            * 1 multiplication for round operation
            * 3 multiplications for masking the clamped value 7 exponential bits with quantization index
            * total = 7 multiplications per input element
        
        * additions (per value of feature map)
            * 1 addition for subtracting the minimum value from clamped value
    
    * dequantization
        
        * multiplications (per value of feature map)
            * 3 multiplications to extract quantization index value and calculate the step that needs to be 
            added to minimum value
        
        * additions
            * 1 addition for adding the minimum value
    * in total, quantization + dequantization need 10 multiplications and 2 additions per input element
    * total number of multiplications performed equals to the multiply of total input elements in feature map and
     total number of multiplications per input element - H\*W\*C\*10
    * total number of additions performed equals to the multiply of total input elements in feature map and
     total number of additions per input element - H\*W\*C\*2
            


## Results

First we trained a normal SimpNet that achieved `81.32%` accuracy using our data-augmentation and training policies. After this we 
binarized the SimpNet without losing accuracy below the `80%` (submission candidate 1) and finally we applied additional quantization 
to the model and reached our second submission candidate.

|         Name          | Accuracy | FLOPS score | Parameters score | Total score  |
|:---------------------:|:--------:|:-----------:|:----------------:|:------------:|
| QuantBinSimpNet       |  80.59   |    0.0595   |      0.0058      |     0.0653   |
| **QuantBinSimpNet+**  |  80.36   |    0.0326   |      0.0058      |     0.0384   |

* Bolded model denotes the best model

The results for Flops, Parameters and Total Micronet Score are normalized against WideResNet-28-10 as described
in the submission instructions. 
